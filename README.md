# SG-MLP
The Official Pytorch Implementation for __Switch Gated Multi-Layer Perceptron(SG-MLP)__.    

SG-MLP, a novel and attentionless architecture for Natural Language Understanding(NLU), achieves decent results in the GLUE benchmark without any help of the Attention Mechanism in both Pre-Training and FineTuning steps. The following repositiory contains demos, pretrained models, and supplementaries necessary for reproducing the results.

<p align="left">
  <img width="446" height="233" src="https://raw.githubusercontent.com/guijinSON/SG-MLP/main/assets/model.png">
</p>

## Masked Language Modeling(MLM) Demo
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/17tMXMTt76uw350toP75d-znSRb0DNLYr?usp=sharing)


## Team  
* 김승원 - [Seungone Kim](https://github.com/SeungoneKim) 
* 손규진 - [GUIJIN SON](https://github.com/guijinSON)
* 주세준 - [Sejune Joo](https://github.com/joocjun)
* 조우진 - [WOOJIN CHO](https://github.com/WooJin-Cho)
* 채형주 - [Hyungjoo Chae](https://github.com/kyle8581)

