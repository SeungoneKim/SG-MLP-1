# SG-MLP
The Official Pytorch Implementation for __Switch Gated Multi-Layer Perceptron(SG-MLP)__.    
  
SG-MLP, a novel and attentionless architecture for Natural Language Understanding(NLU), achieves decent results in the GLUE benchmark without any help of the Attention Mechanism in both Pre-Training and FineTuning steps. The following repositiory contains demos, pretrained models, and supplementaries necessary for reproducing the results.

## Masked Language Modeling(MLM) Demo

## Team  
* 김승원 - [Seungone Kim](https://github.com/SeungoneKim) 
* 손규진 - [GUIJIN SON](https://github.com/guijinSON)
* 주세준 - [Sejune Joo](https://github.com/joocjun)
* 조우진 - [WOOJIN CHO](https://github.com/WooJin-Cho)
* 채형주 - [Hyungjoo Chae](https://github.com/kyle8581)

